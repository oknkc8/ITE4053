{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prac4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEsi5f1m0FWW",
        "colab_type": "text"
      },
      "source": [
        "# Binary Classification using Logistic Regression (Tensroflow 2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slzf-_rS0XXJ",
        "colab_type": "text"
      },
      "source": [
        "### Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auhZf5EzVPvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import easydict\n",
        "\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import pdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0FnaeG70bpU",
        "colab_type": "text"
      },
      "source": [
        "### Set Arguments (not used in ipython)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhslpMxxV8Gj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2814e274-d262-45a9-d37f-525428afcc43"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--m', type=int, default=1000, help='number of train samples')\n",
        "parser.add_argument('--n', type=int, default=100, help='number of test samples')\n",
        "parser.add_argument('--epoch', type=int, default=1000, help='number of epochs')\n",
        "parser.add_argument('--batch', type=int, default=1000, help='batch size')\n",
        "parser.add_argument('--loss', type=str, default='BCE', help='kind of loss function')\n",
        "parser.add_argument('--optimizer', type=str, default='SGD', help='kind of optimizer')\n",
        "parser.add_argument('--log_step', type=int, default=1, help='step for printing log')\n",
        "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--lr'], dest='lr', nargs=None, const=None, default=0.001, type=<class 'float'>, choices=None, help='learning rate', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXmseit9yUyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_generate(m, n):\n",
        "    if os.path.exists('data/') == False:\n",
        "        os.mkdir('data')\n",
        "\n",
        "    x1_train = []\n",
        "    x2_train = []\n",
        "    y_train = []\n",
        "    for i in range(m):\n",
        "        x1_train.append(random.uniform(-2,2))\n",
        "        x2_train.append(random.uniform(-2,2))\n",
        "\n",
        "        if x1_train[-1] * x1_train[-1] > x2_train[-1]:\n",
        "            y_train.append(1)\n",
        "        else:\n",
        "            y_train.append(0)\n",
        "\n",
        "    x_train = np.array((x1_train, x2_train), dtype='float32')\n",
        "    y_train = np.array(y_train, dtype='float32')\n",
        "\n",
        "    if os.path.exists('data/train/') == False:\n",
        "        os.mkdir('data/train')\n",
        "\n",
        "    file_name = 'train_' + str(m) + '_' + str(n)\n",
        "    np.savez('data/train/' + file_name, x_train=x_train, y_train=y_train)\n",
        "\n",
        "\n",
        "    x1_test = []\n",
        "    x2_test = []\n",
        "    y_test = []\n",
        "    for i in range(n):\n",
        "        x1_test.append(random.uniform(-2,2))\n",
        "        x2_test.append(random.uniform(-2,2))\n",
        "\n",
        "        if x1_test[-1] * x1_test[-1] > x2_test[-1]:\n",
        "            y_test.append(1)\n",
        "        else:\n",
        "            y_test.append(0)\n",
        "    \n",
        "    x_test = np.array((x1_test, x2_test), dtype='float32') \n",
        "    y_test = np.array(y_test, dtype='float32')\n",
        "\n",
        "    if os.path.exists('data/test/') == False:\n",
        "        os.mkdir('data/test')\n",
        "\n",
        "    file_name = 'test_' + str(m) + '_' + str(n)\n",
        "    np.savez('data/test/' + file_name, x_test=x_test, y_test=y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh_aumVW1HmC",
        "colab_type": "text"
      },
      "source": [
        "### Main Function\n",
        "Set hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuWMMBe1yYAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  #args = parser.parse_args()\n",
        "  args = easydict.EasyDict({\n",
        "      \"m\" : 1000,\n",
        "      \"n\" : 100,\n",
        "      \"epoch\" : 1000,\n",
        "      \"batch\" : 1000,\n",
        "      \"loss\" : \"BCE\",\n",
        "      \"optimizer\" : \"SGD\",\n",
        "      \"log_step\" : 50,\n",
        "      \"lr\" : 2\n",
        "  })\n",
        "\n",
        "  m = args.m # num of train sample\n",
        "  n = args.n  # num of evaluation sample\n",
        "  epochs = args.epoch\n",
        "  BATCH_SIZE = args.batch\n",
        "  SHUFFLE_BUFFER_SIZE = int(m / 10)\n",
        "  LOSS_FUNCTION = args.loss\n",
        "  OPTIMIZER = args.optimizer\n",
        "  lr = args.lr\n",
        "  log_step = args.log_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf2YcFaG1POF",
        "colab_type": "text"
      },
      "source": [
        "### Load Data from Numpy files\n",
        "Load .npy files or generate data samples when there is no .npy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEj4ekd0yiqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # Load Data\n",
        "  train_file_name = 'data/train/train_' + str(m) + '_' + str(n) + '.npz'\n",
        "  test_file_name = 'data/test/test_' + str(m) + '_' + str(n) + '.npz'\n",
        "\n",
        "  if os.path.exists(train_file_name) == False:\n",
        "      print('Warning! : No Data File')\n",
        "      print('Generating Train & Test set file...')\n",
        "      data_generate(m, n)\n",
        "      print('Done!\\n')\n",
        "\n",
        "  train_set = np.load(train_file_name)\n",
        "  x_train = train_set['x_train'].transpose()\n",
        "  y_train = train_set['y_train'].transpose()\n",
        "\n",
        "  test_set = np.load(test_file_name)\n",
        "  x_test = test_set['x_test'].transpose()\n",
        "  y_test = test_set['y_test'].transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvX76-1w1iOs",
        "colab_type": "text"
      },
      "source": [
        "### Generate Dataset using Dataset Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBpCH7f6zo49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # Generate Dataset\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "  train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "  test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M9D-B0e1qsP",
        "colab_type": "text"
      },
      "source": [
        "### Geneerate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2rEXub3zuym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # Genearte Model\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(3, activation='sigmoid'),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkLNgGOW1ugE",
        "colab_type": "text"
      },
      "source": [
        "### Set Loss Function & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV9I6j7HzyOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # Set Loss Function & Optimizer\n",
        "  if LOSS_FUNCTION == 'BCE':\n",
        "      loss = tf.keras.losses.BinaryCrossentropy()\n",
        "  elif LOSS_FUNCTION == 'MSE':\n",
        "      loss = tf.keras.losses.MeanSquaredError()\n",
        "  if OPTIMIZER == 'SGD':\n",
        "      optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "  elif OPTIMIZER == 'RMS':\n",
        "      optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
        "  elif OPTIMIZER == 'Adam':\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "  train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "  test_loss = tf.keras.metrics.Mean(name='test_loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcGfT2to1zis",
        "colab_type": "text"
      },
      "source": [
        "## Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESGo1NZoz15n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "outputId": "e14049b3-d354-45cf-ce3c-b2f56f48555b"
      },
      "source": [
        "  print(\"######## SETTING ########\")\n",
        "  print(\"num of train sample (m) : %d\" % (m))\n",
        "  print(\"num of test sample (n) : %d\" % (n))\n",
        "  print(\"num of epochs : %d\" % (epochs))\n",
        "  print(\"learning rate : %f\" % (lr))\n",
        "  print(\"batch size : %d\" % (BATCH_SIZE))\n",
        "  print(\"Loss Function : %s\" % (LOSS_FUNCTION))\n",
        "  print(\"Optimizer : %s\" % (OPTIMIZER))\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"######## TRAINING ########\")\n",
        "\n",
        "  # Train & Test\n",
        "  for epoch in range(epochs):\n",
        "      train_acc = 0\n",
        "      test_acc = 0\n",
        "\n",
        "      start_time = time.time()\n",
        "      for x, y in train_dataset:\n",
        "          with tf.GradientTape() as tape:\n",
        "              y_hat = model(x)\n",
        "              t_loss = loss(y, y_hat)\n",
        "          gradients = tape.gradient(t_loss, model.trainable_variables)\n",
        "          optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "          train_loss(t_loss)\n",
        "                    \n",
        "          y_hat = y_hat.numpy()\n",
        "          y_hat[y_hat > 0.5] = 1.0\n",
        "          y_hat[y_hat <= 0.5] = 0.0\n",
        "          train_acc += np.sum(np.expand_dims(y.numpy(), -1) == y_hat)\n",
        "      end_time = time.time()\n",
        "      train_time = end_time - start_time\n",
        "\n",
        "      start_time = time.time()\n",
        "      for test_x, test_y in test_dataset:\n",
        "          y_hat = model(test_x)\n",
        "          t_loss = loss(test_y, y_hat)\n",
        "\n",
        "          test_loss(t_loss)\n",
        "          y_hat = y_hat.numpy()\n",
        "          y_hat[y_hat > 0.5] = 1\n",
        "          y_hat[y_hat <= 0.5] = 0\n",
        "          test_acc += np.sum(np.expand_dims(test_y.numpy(), -1) == y_hat)\n",
        "      end_time = time.time()\n",
        "      test_time = end_time - start_time\n",
        "\n",
        "\n",
        "      if (epoch + 1) % args.log_step == 0:\n",
        "          print('Epoch: %d => train loss: %.6f, train acc: %.3f, train time: %.6f, test loss: %.6f, test acc: %.3f, test time %.6f'\n",
        "              % (epoch+1, train_loss.result(), train_acc/m*100, train_time, test_loss.result(), test_acc/n*100, test_time))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "######## SETTING ########\n",
            "num of train sample (m) : 1000\n",
            "num of test sample (n) : 100\n",
            "num of epochs : 1000\n",
            "learning rate : 2.000000\n",
            "batch size : 1000\n",
            "Loss Function : BCE\n",
            "Optimizer : SGD\n",
            "\n",
            "\n",
            "######## TRAINING ########\n",
            "Epoch: 50 => train loss: 0.431844, train acc: 84.800, train time: 0.009315, test loss: 0.396207, test acc: 84.000, test time 0.005129\n",
            "Epoch: 100 => train loss: 0.335122, train acc: 95.800, train time: 0.008833, test loss: 0.317540, test acc: 94.000, test time 0.004699\n",
            "Epoch: 150 => train loss: 0.272982, train acc: 98.200, train time: 0.009123, test loss: 0.265070, test acc: 96.000, test time 0.004837\n",
            "Epoch: 200 => train loss: 0.231610, train acc: 98.800, train time: 0.008813, test loss: 0.229309, test acc: 97.000, test time 0.004987\n",
            "Epoch: 250 => train loss: 0.202638, train acc: 98.900, train time: 0.009102, test loss: 0.203830, test acc: 98.000, test time 0.004783\n",
            "Epoch: 300 => train loss: 0.181273, train acc: 98.700, train time: 0.009372, test loss: 0.184773, test acc: 98.000, test time 0.005225\n",
            "Epoch: 350 => train loss: 0.164836, train acc: 98.600, train time: 0.009210, test loss: 0.169930, test acc: 98.000, test time 0.004936\n",
            "Epoch: 400 => train loss: 0.151759, train acc: 98.500, train time: 0.009120, test loss: 0.157987, test acc: 98.000, test time 0.004915\n",
            "Epoch: 450 => train loss: 0.141074, train acc: 98.500, train time: 0.008589, test loss: 0.148127, test acc: 98.000, test time 0.004689\n",
            "Epoch: 500 => train loss: 0.132151, train acc: 98.500, train time: 0.009719, test loss: 0.139814, test acc: 99.000, test time 0.004983\n",
            "Epoch: 550 => train loss: 0.124568, train acc: 98.700, train time: 0.010118, test loss: 0.132687, test acc: 99.000, test time 0.005363\n",
            "Epoch: 600 => train loss: 0.118027, train acc: 98.700, train time: 0.008907, test loss: 0.126488, test acc: 99.000, test time 0.004951\n",
            "Epoch: 650 => train loss: 0.112315, train acc: 98.800, train time: 0.009299, test loss: 0.121031, test acc: 100.000, test time 0.005162\n",
            "Epoch: 700 => train loss: 0.107274, train acc: 98.900, train time: 0.011645, test loss: 0.116180, test acc: 100.000, test time 0.006245\n",
            "Epoch: 750 => train loss: 0.102784, train acc: 99.200, train time: 0.009415, test loss: 0.111829, test acc: 100.000, test time 0.005052\n",
            "Epoch: 800 => train loss: 0.098754, train acc: 99.300, train time: 0.009363, test loss: 0.107898, test acc: 100.000, test time 0.005020\n",
            "Epoch: 850 => train loss: 0.095112, train acc: 99.500, train time: 0.008950, test loss: 0.104325, test acc: 99.000, test time 0.004731\n",
            "Epoch: 900 => train loss: 0.091801, train acc: 99.500, train time: 0.009374, test loss: 0.101058, test acc: 99.000, test time 0.005222\n",
            "Epoch: 950 => train loss: 0.088774, train acc: 99.600, train time: 0.010621, test loss: 0.098057, test acc: 99.000, test time 0.005692\n",
            "Epoch: 1000 => train loss: 0.085995, train acc: 99.600, train time: 0.008749, test loss: 0.095288, test acc: 99.000, test time 0.004663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvBraN2B2SlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}